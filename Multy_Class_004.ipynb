{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ca6968-9288-45c2-a71e-9386723df77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602d457d-ba3d-4268-b7ca-dd8ea4ef0431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/ML/\n",
      "Africa\n",
      "China\n",
      "MiddleEAST\n",
      "LatAmerica\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"D:/ML/\"\n",
    "print(DOWNLOAD_ROOT)\n",
    "DATASET_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "DIR_PATH = {\"Africa\":os.path.join(DATASET_PATH, \"Africa\"),\n",
    "            \"China\":os.path.join(DATASET_PATH, \"China\"),\n",
    "            \"MiddleEAST\":os.path.join(DATASET_PATH, \"MiddleEAST\"),\n",
    "           \"LatAmerica\":os.path.join(DATASET_PATH, \"LatAmerica\")\n",
    "           }\n",
    "filenames = {}\n",
    "for items in DIR_PATH.keys():\n",
    "    print(items)\n",
    "    filenames[items] = [name for name in sorted(os.listdir(DIR_PATH[items])) if len(name) > 20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc77ae4-8064-40ec-8174-3e3a2f9a163c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Работаем с txt-файлом\"\"\"\u001b[39;00m        \n\u001b[0;32m     23\u001b[0m spec_chars \u001b[38;5;241m=\u001b[39m string\u001b[38;5;241m.\u001b[39mpunctuation \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m«»\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m—…\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 25\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mfrom_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIR_PATH\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbest())\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     26\u001b[0m content \u001b[38;5;241m=\u001b[39m remove_chars_from_text(content, spec_chars)\n\u001b[0;32m     27\u001b[0m content \u001b[38;5;241m=\u001b[39m remove_chars_from_text(content, string\u001b[38;5;241m.\u001b[39mdigits)\n",
      "File \u001b[1;32mD:\\ML\\ML_env\\Lib\\site-packages\\charset_normalizer\\api.py:548\u001b[0m, in \u001b[0;36mfrom_path\u001b[1;34m(path, steps, chunk_size, threshold, cp_isolation, cp_exclusion, preemptive_behaviour, explain, language_threshold, enable_fallback)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_path\u001b[39m(\n\u001b[0;32m    533\u001b[0m     path: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, PathLike],  \u001b[38;5;66;03m# type: ignore[type-arg]\u001b[39;00m\n\u001b[0;32m    534\u001b[0m     steps: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m     enable_fallback: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CharsetMatches:\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Same thing than the function from_bytes but with one extra step. Opening and reading given file path in binary mode.\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    Can raise IOError.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfrom_fp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m            \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43menable_fallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def remove_chars_from_text(text, chars):\n",
    "    # return \"\".join([ch for ch in text if ch not in chars])\n",
    "    content = ''\n",
    "    for ch in text:\n",
    "        if ch not in chars:\n",
    "            content = content + ''.join(ch)\n",
    "        else:\n",
    "            content = content + ''.join(' ')\n",
    "    return content\n",
    "    \n",
    "\n",
    "x_temp = []\n",
    "y_temp = []\n",
    "X_Africa = []\n",
    "X_Other = []\n",
    "for item in filenames.keys():\n",
    "    post_temp=[]\n",
    "    for names in filenames[item]:\n",
    "        from charset_normalizer import from_path\n",
    "        import string\n",
    "        import os\n",
    "        \"\"\"Работаем с txt-файлом\"\"\"        \n",
    "        spec_chars = string.punctuation + '\\r' + '\\n\\xa0«»\\t—…'\n",
    "               \n",
    "        content = str(from_path(DIR_PATH[item]+\"\\\\\"+names).best()).lower()\n",
    "        content = remove_chars_from_text(content, spec_chars)\n",
    "        content = remove_chars_from_text(content, string.digits)\n",
    "        x_temp.append(content)        \n",
    "        if item=='Africa':\n",
    "            y_temp.append(1)\n",
    "            X_Africa.append(content)\n",
    "        else:\n",
    "            y_temp.append(0)\n",
    "            X_Other.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d407625-708a-4e59-93cf-e6e0bf56fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = np.array(x_temp, dtype=object)\n",
    "y = np.array(y_temp)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cedde20-48e0-4297-80ab-8d4c1b637842",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:7])\n",
    "print(y_train[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62a87b-1f58-4999-b71d-da5a72d86cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6217b83-f1a7-475c-93f0-7dfe5e9912a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import urlextract # may require an Internet connection to download root domain names\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from tokenizer_exceptions import normalizer_exc_rus\n",
    "ru_stopwords = set(\n",
    "        \"\"\"\n",
    "    а авось ага агу аж ай али алло ау ах ая\n",
    "    б будем будет будете будешь буду будут будучи будь будьте бы был была были было\n",
    "    быть бац без безусловно бишь благо благодаря ближайшие близко более больше\n",
    "    будто бывает бывала бывали бываю бывают бытует\n",
    "    в вам вами вас весь во вот все всё всего всей всем всём всеми всему всех всею\n",
    "    всея всю вся вы ваш ваша ваше ваши вдали вдобавок вдруг ведь везде вернее\n",
    "    взаимно взаправду видно вишь включая вместо внакладе вначале вне вниз внизу\n",
    "    вновь вовсе возможно воистину вокруг вон вообще вопреки вперекор вплоть\n",
    "    вполне вправду вправе впрочем впрямь вресноту вроде вряд всегда всюду\n",
    "    всякий всякого всякой всячески вчеред\n",
    "    г го где гораздо гав\n",
    "    д да для до дабы давайте давно давным даже далее далеко дальше данная\n",
    "    данного данное данной данном данному данные данный данных дану данунах\n",
    "    даром де действительно довольно доколе доколь долго должен должна\n",
    "    должно должны должный дополнительно другая другие другим другими\n",
    "    других другое другой\n",
    "    е его едим едят ее её ей ел ела ем ему емъ если ест есть ешь еще ещё ею едва\n",
    "    ежели еле\n",
    "    ж же\n",
    "    з за затем зато зачем здесь значит зря\n",
    "    и из или им ими имъ их ибо иль имеет имел имела имело именно иметь иначе\n",
    "    иногда иным иными итак ишь\n",
    "    й\n",
    "    к как кем ко когда кого ком кому комья которая которого которое которой котором\n",
    "    которому которою которую которые который которым которыми которых кто ка кабы\n",
    "    каждая каждое каждые каждый кажется казалась казались казалось казался казаться\n",
    "    какая какие каким какими каков какого какой какому какою касательно кой коли\n",
    "    коль конечно короче кроме кстати ку куда\n",
    "    л ли либо лишь любая любого любое любой любом любую любыми любых\n",
    "    м меня мне мной мною мог моги могите могла могли могло могу могут мое моё моего\n",
    "    моей моем моём моему моею можем может можете можешь мои мой моим моими моих\n",
    "    мочь мою моя мы мало меж между менее меньше мимо многие много многого многое\n",
    "    многом многому можно мол му\n",
    "    н на нам нами нас наса наш наша наше нашего нашей нашем нашему нашею наши нашим\n",
    "    нашими наших нашу не него нее неё ней нем нём нему нет нею ним ними них но\n",
    "    наверняка наверху навряд навыворот над надо назад наиболее наизворот\n",
    "    наизнанку наипаче накануне наконец наоборот наперед наперекор наподобие\n",
    "    например напротив напрямую насилу настоящая настоящее настоящие настоящий\n",
    "    насчет нате находиться начала начале неважно негде недавно недалеко незачем\n",
    "    некем некогда некому некоторая некоторые некоторый некоторых некто некуда\n",
    "    нельзя немногие немногим немного необходимо необходимости необходимые\n",
    "    необходимым неоткуда непрерывно нередко несколько нету неужели нечего\n",
    "    нечем нечему нечто нешто нибудь нигде ниже низко никак никакой никем\n",
    "    никогда никого никому никто никуда ниоткуда нипочем ничего ничем ничему\n",
    "    ничто ну нужная нужно нужного нужные нужный нужных ныне нынешнее нынешней\n",
    "    нынешних нынче\n",
    "    о об один одна одни одним одними одних одно одного одной одном одному одною\n",
    "    одну он она оне они оно от оба общую обычно ого однажды однако ой около оный\n",
    "    оп опять особенно особо особую особые откуда отнелижа отнелиже отовсюду\n",
    "    отсюда оттого оттот оттуда отчего отчему ох очевидно очень ом\n",
    "    п по при паче перед под подавно поди подобная подобно подобного подобные\n",
    "    подобный подобным подобных поелику пожалуй пожалуйста позже поистине\n",
    "    пока покамест поколе поколь покуда покудова помимо понеже поприще пор\n",
    "    пора посему поскольку после посреди посредством потом потому потомушта\n",
    "    похожем почему почти поэтому прежде притом причем про просто прочего\n",
    "    прочее прочему прочими проще прям пусть\n",
    "    р ради разве ранее рано раньше рядом\n",
    "    с сам сама сами самим самими самих само самого самом самому саму свое своё\n",
    "    своего своей своем своём своему своею свои свой своим своими своих свою своя\n",
    "    себе себя собой собою самая самое самой самый самых сверх свыше се сего сей\n",
    "    сейчас сие сих сквозь сколько скорее скоро следует слишком смогут сможет\n",
    "    сначала снова со собственно совсем сперва спокону спустя сразу среди сродни\n",
    "    стал стала стали стало стать суть сызнова\n",
    "    та то ту ты ти так такая такие таким такими таких такого такое такой таком такому такою\n",
    "    такую те тебе тебя тем теми тех тобой тобою того той только том томах тому\n",
    "    тот тою также таки таков такова там твои твоим твоих твой твоя твоё\n",
    "    теперь тогда тоже тотчас точно туда тут тьфу тая\n",
    "    у уже увы уж ура ух ую\n",
    "    ф фу\n",
    "    х ха хе хорошо хотел хотела хотелось хотеть хоть хотя хочешь хочу хуже\n",
    "    ч чего чем чём чему что чтобы часто чаще чей через чтоб чуть чхать чьим\n",
    "    чьих чьё чё\n",
    "    ш ша\n",
    "    щ ща щас\n",
    "    ы ых ые ый\n",
    "    э эта эти этим этими этих это этого этой этом этому этот этою эту эдак эдакий\n",
    "    эй эка экий этак этакий эх\n",
    "    ю\n",
    "    я явно явных яко якобы якоже\n",
    "    \n",
    "    и что не это  быть этот это свой как - этот весь быть что ▫ но г. %\n",
    "    \"\"\".split()\n",
    "    )\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True, remove_stopwords=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email or \"\"\n",
    "            text = normalizer_exc_rus(text)\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \"\")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', '', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            if self.remove_stopwords:\n",
    "                filtered_words = []\n",
    "                for token in text.split():\n",
    "                    if token not in ru_stopwords:\n",
    "                        filtered_words.append(token)\n",
    " \n",
    "                # Join the filtered words to form a clean text\n",
    "                text = ' '.join(filtered_words)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd63b2b-c8d6-4890-91b9-85d9415242de",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "    url_extractor = urlextract.URLExtract()\n",
    "except ImportError:\n",
    "    url_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca25e26-96dd-4553-b59c-ba95c00a75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import urlextract # may require an Internet connection to download root domain names\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from tokenizer_exceptions import normalizer_exc_rus\n",
    "ru_stopwords = set(\n",
    "        \"\"\"\n",
    "    а авось ага агу аж ай али алло ау ах ая\n",
    "    б будем будет будете будешь буду будут будучи будь будьте бы был была были было\n",
    "    быть бац без безусловно бишь благо благодаря ближайшие близко более больше\n",
    "    будто бывает бывала бывали бываю бывают бытует\n",
    "    в вам вами вас весь во вот все всё всего всей всем всём всеми всему всех всею\n",
    "    всея всю вся вы ваш ваша ваше ваши вдали вдобавок вдруг ведь везде вернее\n",
    "    взаимно взаправду видно вишь включая вместо внакладе вначале вне вниз внизу\n",
    "    вновь вовсе возможно воистину вокруг вон вообще вопреки вперекор вплоть\n",
    "    вполне вправду вправе впрочем впрямь вресноту вроде вряд всегда всюду\n",
    "    всякий всякого всякой всячески вчеред\n",
    "    г го где гораздо гав\n",
    "    д да для до дабы давайте давно давным даже далее далеко дальше данная\n",
    "    данного данное данной данном данному данные данный данных дану данунах\n",
    "    даром де действительно довольно доколе доколь долго должен должна\n",
    "    должно должны должный дополнительно другая другие другим другими\n",
    "    других другое другой\n",
    "    е его едим едят ее её ей ел ела ем ему емъ если ест есть ешь еще ещё ею едва\n",
    "    ежели еле\n",
    "    ж же\n",
    "    з за затем зато зачем здесь значит зря\n",
    "    и из или им ими имъ их ибо иль имеет имел имела имело именно иметь иначе\n",
    "    иногда иным иными итак ишь\n",
    "    й\n",
    "    к как кем ко когда кого ком кому комья которая которого которое которой котором\n",
    "    которому которою которую которые который которым которыми которых кто ка кабы\n",
    "    каждая каждое каждые каждый кажется казалась казались казалось казался казаться\n",
    "    какая какие каким какими каков какого какой какому какою касательно кой коли\n",
    "    коль конечно короче кроме кстати ку куда\n",
    "    л ли либо лишь любая любого любое любой любом любую любыми любых\n",
    "    м меня мне мной мною мог моги могите могла могли могло могу могут мое моё моего\n",
    "    моей моем моём моему моею можем может можете можешь мои мой моим моими моих\n",
    "    мочь мою моя мы мало меж между менее меньше мимо многие много многого многое\n",
    "    многом многому можно мол му\n",
    "    н на нам нами нас наса наш наша наше нашего нашей нашем нашему нашею наши нашим\n",
    "    нашими наших нашу не него нее неё ней нем нём нему нет нею ним ними них но\n",
    "    наверняка наверху навряд навыворот над надо назад наиболее наизворот\n",
    "    наизнанку наипаче накануне наконец наоборот наперед наперекор наподобие\n",
    "    например напротив напрямую насилу настоящая настоящее настоящие настоящий\n",
    "    насчет нате находиться начала начале неважно негде недавно недалеко незачем\n",
    "    некем некогда некому некоторая некоторые некоторый некоторых некто некуда\n",
    "    нельзя немногие немногим немного необходимо необходимости необходимые\n",
    "    необходимым неоткуда непрерывно нередко несколько нету неужели нечего\n",
    "    нечем нечему нечто нешто нибудь нигде ниже низко никак никакой никем\n",
    "    никогда никого никому никто никуда ниоткуда нипочем ничего ничем ничему\n",
    "    ничто ну нужная нужно нужного нужные нужный нужных ныне нынешнее нынешней\n",
    "    нынешних нынче\n",
    "    о об один одна одни одним одними одних одно одного одной одном одному одною\n",
    "    одну он она оне они оно от оба общую обычно ого однажды однако ой около оный\n",
    "    оп опять особенно особо особую особые откуда отнелижа отнелиже отовсюду\n",
    "    отсюда оттого оттот оттуда отчего отчему ох очевидно очень ом\n",
    "    п по при паче перед под подавно поди подобная подобно подобного подобные\n",
    "    подобный подобным подобных поелику пожалуй пожалуйста позже поистине\n",
    "    пока покамест поколе поколь покуда покудова помимо понеже поприще пор\n",
    "    пора посему поскольку после посреди посредством потом потому потомушта\n",
    "    похожем почему почти поэтому прежде притом причем про просто прочего\n",
    "    прочее прочему прочими проще прям пусть\n",
    "    р ради разве ранее рано раньше рядом\n",
    "    с сам сама сами самим самими самих само самого самом самому саму свое своё\n",
    "    своего своей своем своём своему своею свои свой своим своими своих свою своя\n",
    "    себе себя собой собою самая самое самой самый самых сверх свыше се сего сей\n",
    "    сейчас сие сих сквозь сколько скорее скоро следует слишком смогут сможет\n",
    "    сначала снова со собственно совсем сперва спокону спустя сразу среди сродни\n",
    "    стал стала стали стало стать суть сызнова\n",
    "    та то ту ты ти так такая такие таким такими таких такого такое такой таком такому такою\n",
    "    такую те тебе тебя тем теми тех тобой тобою того той только том томах тому\n",
    "    тот тою также таки таков такова там твои твоим твоих твой твоя твоё\n",
    "    теперь тогда тоже тотчас точно туда тут тьфу тая\n",
    "    у уже увы уж ура ух ую\n",
    "    ф фу\n",
    "    х ха хе хорошо хотел хотела хотелось хотеть хоть хотя хочешь хочу хуже\n",
    "    ч чего чем чём чему что чтобы часто чаще чей через чтоб чуть чхать чьим\n",
    "    чьих чьё чё\n",
    "    ш ша\n",
    "    щ ща щас\n",
    "    ы ых ые ый\n",
    "    э эта эти этим этими этих это этого этой этом этому этот этою эту эдак эдакий\n",
    "    эй эка экий этак этакий эх\n",
    "    ю\n",
    "    я явно явных яко якобы якоже\n",
    "    \n",
    "    и что не это  быть этот это свой как - этот весь быть что ▫ но г. %\n",
    "    \"\"\".split()\n",
    "    )\n",
    "# stemmer = nltk.PorterStemmer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "def Make_dict(X_text, strip_headers=True, lower_case=True,\n",
    "    remove_punctuation=True, replace_urls=True, replace_numbers=True,\n",
    "    stemming=True, remove_stopwords=True):\n",
    "    \n",
    "    X_transformed = []\n",
    "    for email in X_text:\n",
    "        text = email or \"\"\n",
    "        text = normalizer_exc_rus(text)\n",
    "        if lower_case:\n",
    "            text = text.lower()\n",
    "        if replace_urls and url_extractor is not None:\n",
    "            urls = list(set(url_extractor.find_urls(text)))\n",
    "            urls.sort(key=lambda url: len(url), reverse=True)\n",
    "            for url in urls:\n",
    "                text = text.replace(url, \"\")\n",
    "        if replace_numbers:\n",
    "            text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', '', text)\n",
    "        if remove_punctuation:\n",
    "            text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "        if remove_stopwords:\n",
    "            filtered_words = []\n",
    "            for token in text.split():\n",
    "                if token not in ru_stopwords:\n",
    "                    filtered_words.append(token)\n",
    "    \n",
    "            # Join the filtered words to form a clean text\n",
    "            text = ' '.join(filtered_words)\n",
    "        word_counts = Counter(text.split())\n",
    "        if stemming and stemmer is not None:\n",
    "            stemmed_word_counts = Counter()\n",
    "            for word, count in word_counts.items():\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                stemmed_word_counts[stemmed_word] += count\n",
    "            word_counts = stemmed_word_counts\n",
    "        X_transformed.append(word_counts)\n",
    "    return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0d681-f40e-4eff-b142-3e5d470d8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = Make_dict(X_train[:7])\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d40ae4-4afb-4bf4-a50d-e7de53ad40d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Africa_dic = Make_dict(X_Africa)\n",
    "X_Africa_words = []\n",
    "for post in X_Africa_dic:\n",
    "    for word in post.keys():\n",
    "        X_Africa_words.append(word)\n",
    "print(X_Africa_words [:7]) \n",
    "\n",
    "\n",
    "X_Other_dic = Make_dict(X_Other)\n",
    "X_Other_words = []\n",
    "for post in X_Other_dic:\n",
    "    for word in post.keys():\n",
    "        X_Other_words.append(word)\n",
    "print(X_Other_words [:7]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228423a-6751-4038-925b-694f6b99c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Africa_words_ex = []\n",
    "for word in X_Africa_words:\n",
    "    if word not in X_Other_words:\n",
    "        Africa_words_ex.append(word)\n",
    "print(Africa_words_ex[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d500fa9-2a20-40ee-9016-69d63a29521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(Africa_words_ex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af4209-af21-402b-ab9c-5743d5e70e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Other_words_ex = []\n",
    "for word in X_Other_words:\n",
    "    if word not in X_Africa_words:\n",
    "        Other_words_ex.append(word)\n",
    "print(Other_words_ex[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddefba-11d5-40ab-bee6-96275c3c0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(Other_words_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f7d2c-bbcb-4023-9d7a-ddcddc3bdecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "features = dictvectorizer.fit_transform(data_dict)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2a963-a9f6-4793-b471-a7e06801c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = dictvectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29412511-4aac-48f5-ae77-2667b9255756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=4000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236928cd-ae40-4821-a57b-b00e7f0d1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer(vocabulary_size=7000)),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1292c80-0f1a-4dd1-bd19-c581b593f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=6000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f2eaf-6cdb-4aba-bf8a-0a75df016107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=6000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred, average='micro')))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd10c33-eb3d-4fc1-8cbf-f4d1fbc547e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ['Нигер последовал примеру Мали, разорвав связи с Украиной после того, как пресс-секретарь разведывательного агентства Министерства обороны Украины признал, что Украина оказывала поддержку повстанцам, которые убили малийских вооруженных сил и российских агентов 25-27 июля в Тинзауатене, на севере Мали, недалеко от границы с Алжиром.Посол Украины в Сенегале также выразил безоговорочную поддержку малийским повстанцам, что привело к вызову посла в МИД Сенегала.',\n",
    "            'Еще новости афганской промышленности. В Кандагаре спустя 18 лет возобновила работу текстильная фабрика. Официальные лица говорят, что они отремонтировали оборудование на фабрике за шесть месяцев, сообщают афганское СМИ. По словам работников фабрики, с возобновлением работы фабрики созданы рабочие места. 69-летний Мохаммад, проработавший на текстильной фабрике в Кандагаре значительное время, рад вернуться на работу.',\n",
    "           'Накопленное непонимание между Китаем и США не может быть решено лишь одним подобным изящным жестом. Страны расходятся по широкому кругу вопросов, санкционный механизм против Китая не ослаблен. Поэтому отправка панд – это скорее демонстрация миролюбивой позиции Китая в противовес США.',\n",
    "            'В Алжире, как правило, на похоронах мужчины сидят около дома, хорошо, если есть сад или терраса, а женщины в доме, надеть платок женщина должна обязательно, даже если в обычной жизни она не покрывает голову. Если покойный жил в квартире, то прямо во дворе ставят стулья и шатры со столами для мужчин.Готовят либо нанятые кухарки, либо родственницы покойного. Пока составляла пост от подруги услышала,что в их семье часто еду приносят те, кто приходит в дом, а готовить должны невестки, а не дочери умершего. Мой муж сказал, что это совершенно необязательно, на похоронах его бабушки готовили только её дочери, это было их желание и никаких особых правил на счёт этого нет. Обязательного блюда на поминки в Алжире нет. Народ приходит помянутьот трех дней до недели, если у человека была большая семья и много знакомых!Кормят традиционными блюдами: суп-шорба, кус-кус/беркукес/тлитли с мясом и т.д.Могилу посещают каждое утро, в течении трех дней. Через 40 дней устанавливают небольшое надгробие, никаких памятников,вычурных элементов. На надгробии имя, фамилия, даты рождения и смерти, фотографий нет. Вроде бы написала всё, что знала. Задавайте вопросы в комментариях, если таковые имеются.',\n",
    "            'Skyeton K-10 Swift - упал У с. Лесновичи в Львовской области… Геопривязка: (49.853000, 23.570500) Курсант ХНУВС разбился в ходе вылета на этом учебно тренировочном самолете… Датировано: 27.07.2024.',\n",
    "            'Сегодня в Венесуэле проходят президентские выборы. Мой коллега Дмитрий Морозов рассуждает, как их итоги могут повлиять на дальнейший путь страны. Вот самое важное из его статьи.📍В выборах один тур, участвуют десять кандидатов, однако только двое имеют реальные шансы на победу: действующий президент Николас Мадуро и Эдмундо Гонсалес Уррутия, кандидат от Единой демократической платформы, объединяющей наиболее значимые оппозиционные партии.'\n",
    "           ]\n",
    "accuracy = log_clf.predict(preprocess_pipeline.transform(new_text))\n",
    "\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a187ee04-4617-47bd-92bf-3b2cb9b2961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('churn-model.bin', 'wb') as f_out:\n",
    "    pickle.dump(log_clf, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e73a07-c11f-4c10-ac05-c237120be0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c9d79-0357-4dad-a170-6dafd298b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "preprocess_pipeline = Pipeline([\n",
    "        (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "        (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "    ])\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)\n",
    "for depth in [1, 2, 3, 4, 5, 6, 10, 15, 20, 30, 45, 60, None]:\n",
    "    rf = DecisionTreeClassifier(max_depth=depth , min_samples_leaf=50)\n",
    "    rf.fit(X_train_transformed, y_train)\n",
    "    y_pred = rf.predict_proba(X_train_transformed)[:, 1]\n",
    "    auc = roc_auc_score(y_train, y_pred)\n",
    "    print('%4s -> %.3f' % (depth, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381d32d-af91-4b87-bb86-e6aef9d07010",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [4, 10, 30]:\n",
    "    print('depth: %s' % m)\n",
    "    for s in [10, 50, 100, 150, 200, 300, 400, 500]:\n",
    "        rf = DecisionTreeClassifier(max_depth=m, min_samples_leaf=s)\n",
    "        rf.fit(X_train_transformed, y_train)\n",
    "        y_pred = rf.predict_proba(X_train_transformed)[:, 1]\n",
    "        auc = roc_auc_score(y_train, y_pred)\n",
    "        print('%4s -> %.3f' % (s, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684e4a2-6825-4b4a-92f0-c1b572ed21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "rf = DecisionTreeClassifier(max_depth=10, min_samples_leaf=300)\n",
    "rf.fit(X_train_transformed, y_train)\n",
    "y_pred = rf.predict_proba(X_test_transformed)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print('%.3f' % ( auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9240a6f-6061-4c4f-9d93-cb3f7912e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "for depth in [1, 2, 3, 4, 5, 6, 10, 15, 20, 30, 45, 60, None]:\n",
    "    rf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=50)\n",
    "    rf.fit(X_train_transformed, y_train)\n",
    "    y_pred = rf.predict_proba(X_test_transformed)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print('%4s -> %.3f' % (depth, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df244e9d-a5cb-4763-a6b7-787ab021c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [10, 15, 20]:\n",
    "    print('depth: %s' % m)\n",
    "    for s in [10, 50, 100, 150, 200, 300, 400, 500]:\n",
    "        rf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=s)\n",
    "        rf.fit(X_train_transformed, y_train)\n",
    "        y_pred = rf.predict_proba(X_test_transformed)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        print('%4s -> %.3f' % (s, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b83721-d46e-46dd-a043-ee885a7dc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "preprocess_pipeline = Pipeline([\n",
    "        (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "        (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "    ])\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)\n",
    "\n",
    "aucs = []\n",
    "for i in range(10, 201, 10):\n",
    "    rf = RandomForestClassifier(n_estimators=i, random_state=3)\n",
    "    rf.fit(X_train_transformed, y_train)\n",
    "    y_pred = rf.predict_proba(X_train_transformed)[:, 1]\n",
    "    auc = roc_auc_score(y_train, y_pred)\n",
    "    print('%s -> %.3f' % (i, auc))\n",
    "    aucs.append(auc)\n",
    "plt.plot(range(10, 201, 10), aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca788c7-3d49-4756-96b5-d4b9615badd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
